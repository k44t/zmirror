--- !ZMirror


# enable logging of the environment variables for all udev and zed events
log_events: yes

# If set to `no`, then zmirror never executes any commands 
# but only logs the commands that it would execute. This is helpful when you
# want to test things and/or manually interact with the devices without zmirror 
# interfering
#
# you can temporarily override this at runtime by running `zmirror enable-commands` or `zmirror disable-commands`
enable_commands: no

# the timeout for user requests after which the request will be considered failed 
# and removed.
#
# Please note that this timeout will not affect requests that can be fulfilled
# at a later time in respect to the current state. For example if you request a `scrub`
# while a `resilver` is in progress, then zmirror considers the `scrub` to be 
# fulfillable once the `resilver` has finished. The same holds for `trim`.
#
# A `scrub` request however may be cancelled after the timeout, if the device is currently
# offline and the `online` request that is automatically scheduled when you schedule a 
# `scrub` cannot be fulfilled before the timeout ends.
timeout: 60






# zmirror itself does not support scheduling when the maintenance task should be 
# run (`zmirror maintenance`), since there are great tools
# that do just that. Basically you create a systemd service that runs at whatever
# interval you prefer and that calls: `zmirror scrub-all` or `zmirror trim-all` (depending on whether you configured all your disks
# to always be trimmed after a scrub or no)
# 
# You can of course always do `zpool scrub-all` or `zpool trim-all` directly. 
#
# Please note that zmirror should always be running/restarted with 
# `disable_commands: yes` when you do anything manual, so that it can ubdate its disk state information 
# accordingly.

# sets the default trim interval. May be overridden by setting `trim_interval` 
# directly on a device
trim_interval: 1 week

# sets the default trim interval. May be overridden by setting `scrub_interval` 
# directly on a device
# uncomment to enable
scrub_interval: 4 weeks


# under here all top-level entities (such as disks, partitions, lvm volume groups
# or zpools) are configured
content:


  # This zpool contains my root filesystem. It also contains my most important data (my projects)
  #
  # the configuration below displays my mirroring and backup strategy.
  - !ZPool 
    name: zmirror-sysfs

    # the sysfs pool must be imported during boot so zmirror should not import it

    # but we still configure the backing so that zmirror knows 
    backed_by:
      - !Mirror
        devices: 
          # without zmirror-big-a present zmirror should not import the pool
          - name: zmirror-sysfs-a
            required: yes
          
          - name: zmirror-sysfs-b
          
          - name: zmirror-sysfs-s
            
            # zmirror should not automatically open the DMCrypt
            # to online the device (only when zmirror-sysfs-s needs 
            # scrubbing will zmirror try to online all of its dependencies)
            online_dependencies: no

          - name: zvol/zmirror-bak-a/sysfs
            online_dependencies: no

          - name: zvol/zmirror-bak-b/sysfs
            online_dependencies: no



  # the disk zmirror-sysfs-a is one of the disks that back the zmirror-sysfs zpool
  # it is an NVMe drive built into my computer
  # and this is the partition (on the drive) that contains a mirror of the data of the zmirror-sysfs-zpool
  #
  # in this zmirror configuration I configure partitions at the top level. It is
  # possible to configure disks and then have the partition inside the `content:`
  # element of the disks. This has not much effect on the working of zmirror,
  # except that the disk itself would appear in its status output. It has absolutely
  # no bearing on any functionality of zmirror itself, since the kernel automatically
  # creates udev events whenever partitions appear due to a disk being inserted.
  - !Partition

    # IMPORTANT: partition names that will be used by zmirror MUST be unique
    name: zmirror-sysfs-a
    
    content:


      # we use DMCrypt over ZFS native encryption for two reasons:
      #
      # -a) while it encrypts file content, it does not encrypt filenames
      #   and other metadata
      # -b) zfs send/receive currently corrupts data from encrypted datasets
      #   (see https://github.com/openzfs/zfs/issues/12014)
      #
      - !DMCrypt

        # IMPORT: the names of DM crypts in your system MUST be unique
        name: zmirror-sysfs-a
        
        # zmirror can only work with key-files as of now.
        key_file: ./test/zmirror-key
        # if somebody was willing to write a python function
        # that acquires the key either from the user (systemd-ask-password)
        # or from some encryption system (smart card?) you
        # are very welcome. The zmirror daemon runs as root.

        # the parent-child relationship is always directly visible in this
        # configuration. I.e. the parent of this DMCrypt *is* the partition
        # and the children of this DMCrypt (or rather the only child in this
        # case)
        on_appeared:
          - online

        # when all children are offline (in this case there is only one,
        # namely one of the mirrors that backs the zmirror-sysfs zpool)
        # thes this dmcrypt should be closed
        on_children_offline: 
        
          # synonm: offline == close
          - offline

        content:
          
          # this might be considered a virtual entity. It basically tells zmirror
          # that the data inside the partition is the data backing the zpools
          # (or in other words, that the partition is the blockdev that backs
          # the zpool)
          #
          # zmirror currently only supports `ZDev`s to reside
          # inside `Partition`s, `DMCrypt`s or `ZFSVolume`s. This is likely not going to change
          # because zmirror relies on unique names, and raw disks do not have names
          # while partitions and dm_crypts CAN have unique names (and MUST have if they
          # are to be used within a zmirror configuration) .
          #
          # This means, when you connect unknown devices, you should disable zmirror-daemon
          # while playing around with them for it might mess with zmirror's 
          # status cache or even trigger zmirror executing unwanted commands.
          - !ZDev
            # name for the zfs pool
            pool: zmirror-sysfs

            # when it appears we want to take the device online IF and only if 
            # the corresponding zpool is online.
            #
            # we could have used `online` as well, but then zmirror would try and
            # bring the corresponding zpool online. Onlining zpools however
            # we configure via on_backing_appeared on the zpool itself.
            on_appeared:
              - online

            on_scrubbed:
              - trim
              
            

            scrub_interval: 2 weeks



  # the disk zmirror-sysfs-b is an NVMe disk that is connected to my computer
  # via USB-C
  # it serves as a failover in case zmirror-sysfs-a has a hardware error
  # 
  # the only reason it appears in this configuration, is that we need to force-enable TRIM
  - !Disk

    # the uuid of the gpt partition table
    uuid: 00000000-0000-0000-0000-000000000002

    # since disks have no names, the info field will be printed in zmirror logs for better usability
    info: zmirror-sysfs-b

    # *WARNING*: ONLY set this to true if you absolutely know that your device supports trim
    #
    # Force enable trim support on SSD devices/enclosures that actually do support trim.
    #
    # The TRIM command is used by solid state devices to clear datablocks and 
    # mark them unwritten, which can greatly enhance disk performance.
    # 
    # the linux kernel is not aware that my usb-c NVMe enclosure does in fact
    # support the trim command.
    #
    #To see whether the kernel has recognized trim support on your device you can run (replacing <sdX> with an your disk identifier):
    #
    # :: cat /sys/block/<sdX>/device/scsi_disk/0:0:0:0/provisioning_mode
    # 
    # >> full
    #
    # If the output is "full", then the kernel did NOT recognize trim support. If it is "unmap" then it was recognized.
    #
    # 
    # whether your device does support trim, you can find out using
    #
    # :: sg_vpd -a /dev/sdX | grep "map"
    # 
    #   
    # A report like
    #
    # >> 
    #   Maximum unmap LBA count: 0 [Unmap command not implemented]
    #   Maximum unmap block descriptor count: 0 [Unmap command not implemented]
    #   Optimal unmap granularity: 0 blocks [not reported]
    #
    # means that trim is not supported, so stop there. 
    #
    # A report like:
    #
    # >>  
    #   Maximum unmap LBA count: 0x1400000
    #   Maximum unmap block descriptor count: 0x1
    #   Optimal unmap granularity: 0x1
    # 
    # means that trim is supported.
    # 
    # note that you also need to manually `--allow-discards` for all dm-crypts on this disk
    # (please inform youself on the security implications of doing so)
    #
    # This following command will persistently `--allow-discards` for an already open dm-crypt
    #
    # :: cryptsetup --allow-discards --persistent refresh zmirror-sysfs-a --key-file ./test/zmirror-key
    #
    # you can verify `--allow-discards` status with:
    #
    # :: dmsetup table zmirror-sysfs-a
    #
    # *WARNING*: ONLY set this to true if you absolutely know that your device supports
    # trim.
    force_enable_trim: yes
    # 
    # what zmirror does to enable trim is catch every udev add or change event for 
    # the disk and do
    # 
    # :: echo "unmap" > /sys/block/sdX/device/scsi_disk/0:0:0:0/provisioning_mode



    content:

    - !Partition

      # the partition name must be unique even if you configure a partition as the content
      # of a disk in zmirror. This is because we still rely on udev events and those do
      # not contain the disk to which a udev event belongs. 
      #
      # unique names also always make debugging your configuration easier.
      name: zmirror-sysfs-b

      
      content:
        - !DMCrypt

          name: zmirror-sysfs-b
          
          # at this point zmirror only supports key_file based encryption with `dm_crypt`
          key_file: ./test/zmirror-key

          on_children_offline: 
            - offline

          on_appeared:
            - online

          content:
            - !ZDev
            
              pool: zmirror-sysfs

              on_appeared: 
                - online

              # after the disk has been scrubbed we want to trim it to preserve performance
              on_scrubbed:
                - trim

              scrub_interval: 4 weeks





  # the disk zmirror-sysfs-c is equal in hardware and configuration to
  # zmirror-sysfs-b.
  #
  # I switch out c and b every other day. Their task is to mirror the
  # internal drive and be a failover. Having only one of them connected
  # at any one time means, that I have a full and recent copy of my data
  # in case an overvoltagecondition destroys my PC and both the internal
  # NVMe drive and the external NVMe drive that is currently connected.
  #
  # Both zmirror-sysfs-b and zmirror-sysfs-c operate in full PCIe
  # speed, so they are just as fast as the internal NVMe drive.
  - !Disk

    # the uuid of the gpt partition table. zmirror currently only supports GPT formatted disks.
    uuid: 00000000-0000-0000-0000-000000000003

    # since disks have no names, the info field will be printed in zmirror logs for better usability
    info: zmirror-sysfs-c

    # only enable this if you are absolutely certain that your NVMe enclosure
    # supports TRIM. See above for more.
    force_enable_trim: yes


    content:

    - !Partition

      # the partition name must be unique even if you configure a partition as the content
      # of a disk in zmirror. This is because we still rely on udev events and those do
      # not contain the disk to which a udev event belongs. 
      #
      # unique names also always make debugging your configuration easier.
      name: zmirror-sysfs-c

      
      content:
        - !DMCrypt

          name: zmirror-sysfs-c
          
          key_file: ./test/zmirror-key

          on_children_offline: 
            - offline

          on_appeared:
            - online

          content:
            - !ZDev
            
              pool: zmirror-sysfs

              on_appeared: 
                - online

              on_scrubbed:
                - trim

              scrub_interval: 4 weeks



  # zmirror-sysfs-s
  #
  # The sysfs zpool contains (in addition to the OS and config files) my most 
  # important projects files. Hence I have a 2TB nvme drive named zmirror-sysfs-s
  # (s stands for "s"afe) which I carry around with myself and which I connect 
  # when I'm working on my computer.
  # 
  # This NVMe drive is a different spec (2242) than sysfs-a, b and c. It has a smaller
  # form factor, so it fits into a smaller USB drive enclosure. The disadvantage is, that 
  # it is also slower than the other three. Hence I take it offline after a resilver.
  # 
  # Risk-Mitigation: losing my most recent changes due to a break-in while I'm away.
  #
  # This is the partition that contains one of the zpool's (zmirror-sysfs)
  # mirrors.
  - !Disk

    # the uuid of the gpt partition table
    uuid: 00000000-0000-0000-0000-000000000004

    # since disks have no names, the info field will be printed in zmirror logs for better usability
    info: zmirror-sysfs-s

    # only enable this if you are absolutely certain that your NVMe enclosure
    # supports TRIM. See above for more.
    force_enable_trim: yes

    content:
      - !Partition

        name: zmirror-sysfs-s

        
        content:
          - !DMCrypt

            name: zmirror-sysfs-s
            
            key_file: ./test/zmirror-key

            on_children_offline: 
              - offline

            on_appeared:
              - online

            content:
              - !ZDev
              
                pool: zmirror-sysfs

                on_appeared: 
                  - online
                
                # This NVMe drive is a different spec (2242) than sysfs-a, b and c. It has a smaller
                # form factor, so it fits into a smaller USB drive big encloser. But it also is 
                # slower (PCIe 1.0) than the other three. Hence I take it offline after a resilver.
                on_resilvered:
                  - offline

                on_scrubbed:
                  - trim
                
                # because of the slower speed (same reason as above) I take it offline when the operation
                # is done.
                on_trimmed:
                  - offline

                scrub_interval: 4 weeks






  - !ZPool
    name: zmirror-big



    # This zpool should be taken online if all necessary backing devices
    # have appeared. 
    # 
    # This will happen once I have powered the devices on via the shell
    # command, as the respective udev events are being sent by the kernel.
    #
    # *WARNING*: This configuration is dangerous:
    #
    # ZFS uses the first disk that is present when the pool was imported as its
    # references point. If you then connect another disk, that was not yet
    # present at pool import time, and if that other disk contained newer 
    # data because it was last used, then that newer data WILL BE LOST.
    # 
    # So you should make sure that you always connect the disk first, that 
    # has the newer data.
    # 
    # In my case, one of the disks backing zmirror-big is an internal SATA
    # disk. This is my master disk, and it is always present and always
    # contains the newest data. So I can never run into this issue.
    # This internal disk is also marked as required within the `Mirror` definition
    # so zmirror will not import the pool if it is not present.
    on_backing_appeared:
      - online
    
    # for above event handling to work, we also need to configure the backing:
    backed_by:
      - !Mirror
        devices: 
          # without zmirror-big-a present zmirror should not import the pool
          - name: zmirror-big-a
            required: yes
          - zmirror-big-b
          - zvol/zmirror-bak-a/big
          - zvol/zmirror-bak-b/big






  - !Partition

    name: zmirror-big-a

    content:
      - !DMCrypt

        name: zmirror-big-a
        
        key_file: ./test/zmirror-key

        on_children_offline: 
          - offline
        
        on_appeared:
          - online

        content:
          - !ZDev
          

            pool: zmirror-big
              
            on_appeared:
              - online

            scrub_interval: 4 weeks






  - !Partition

    name: zmirror-big-b

    
    content:
      - !DMCrypt

        name: zmirror-big-b
        
        key_file: ./test/zmirror-key

        on_children_offline: 
          - offline
        
        on_appeared:
          - online

        content:

          - !ZDev
          

            pool: zmirror-big
          

            on_appeared:
              - online

            scrub_interval: 4 weeks






  # this is one of the multiple backup pools (only two are configured in this
  # example) which contains zfs volumes that mirror the zpools zmirror-sysfs 
  # and zmirror-big.
  #
  # These pools exist for backup purposes only.
  # zmirror creates ZFS snapshots to save the state after every backup (I need to
  # manually remove some of the older ones every now and then to ensure I have 
  # enough space).
  # 
  # This kind of configuration gives me real backups.
  #
  # Risk-Mitigation
  # 
  # - internal disk failure
  # - always connected external disk failure
  # - power surge (failure of server and all connected disks)
  # - EMP (when disconnected I store the disks in EMP protected bags, however
  #   mainly to mitigate bitrot)
  # - bitrot
  # - local catastrophe (depends on how I store the backing disks, see below)
  # 
  # The only risks that are not mitigated by this are 
  # 
  # -1. bugs in ZFS that make me lose some or all of the data inside the backups.
  # -2. hackers that maliciously delete my backup data WHILE the backup disk
  #   is connected
  # 
  # Ad 1. I feel the risk of ZFS having or introducing such bugs and my OS
  # distributing such bugs into the stable release is very low, too low in fact
  # for me to worry about them. (I do not use ZFS native encryption which has 
  # such bugs.)
  # 
  # Ad 2. can only be mitigated by using backup media that will be written once
  # and only be read again in case of needing to restore. An example would be 
  # burning blurays (Millenium Disks). Another example would be magnetic tape.
  # 
  # You could combine zmirror with magnetic tape. But it would probably make more
  # sense to use a simpler filesystem for magnetic tapes which you will always
  # be able to restore if you have the relevant hardware (while it might be quite
  # difficult to retrive data from an old ZFS version or the like). So zmirror's
  # functionality would then NOT be for you.
  # 
  # In any case, the kind of failure protection this zmirror configuration gives me
  # is enough for my own needs.
  #
  #
  - !ZPool
    name: zmirror-bak-a

    # when all volumes configured below are offline (WARNING: see explanation 
    # below on how we implemented this) the zpool will be exported.
    # 
    # WARNING: this kind of configuration only makes sense if you use the
    # zpool zmirror-bak-a ONLY as a container for volumes that are managed
    # by zmirror. In any other case you want to tell zmirror to never
    # export the pool and instead handle exporting manually
    on_children_offline: 
      - offline
    

    # This zpool should be taken online if all necessary backing devices
    # have appeared. 
    # 
    # This will happen once I have powered the devices on via the shell
    # command, as the respective udev events are being sent by the kernel.
    # 
    # This configuration is not dangerous, because the backing layout does 
    # not contain a mirror or a raid-z
    on_backing_appeared:
      - online
    
    # for above event handling to work, we also need to configure the backing layout:
    backed_by:
      - "zmirror-bak-a"





    content:
      
      # since zmirror needs udev/zed events to process state changes we use
      # the fact that we can make zvols (in-) visible to the system to generate
      # those events. In other words for taking the volume offline zmirror issues
      #
      # :: zfs set volmode=none zmirror-bak-a/sysfs
      # 
      # which makes the volume disappear as a blockdev. Then we catch the 
      # udev event and then we consider the volume offline.
      # 
      # it is important to note that the blockdev will only disappear when
      # it is not in use anymore. Only then will the udev events be sent and
      # only then can zmirror act apropriately.
      # 
      # When both `zmirror-bak-a/sysfs` and `zmirror-bak-a/big` are (cosidered)
      # offline, zmirror will export the pool (as configured above).
      - !ZFSVolume
        name: sysfs

        on_children_offline:
          - offline
        
        # when the zpool zmirror-bak-a comes online, zmirror will issue the
        # the command:
        #
        # :: zfs set volmode=full zmirror-bak-a/sysfs
        # 
        # which causes the volume to become visible to the system like a any
        # other blockdev (e.g. /dev/zd224)
        # 
        # Then zmirror catches the respective udev event and changes its
        # internal state of the volume to online.
        on_appeared:
          - online
        
        content:


          # to be able to create such a disk layout as this, we have to trick zfs
          # into not partitioning the blockdev /dev/zvol/zmirror-bak-a/sysfs.
          #
          # Not partitioning the volume is important because otherwise we 
          # would have to make zmirror's configuration more complicated and
          # add the partitions to it.
          #
          # To trick zfs we use an unused loop device (in my case loop0 is unused):
          #
          # :: losetup /dev/loop0 /dev/zvol/zmirror-bak-a/sysfs
          #
          # and then we attach the disk to the pool
          # 
          # :: zpool attach zmirror-sysfs zmirror-sysfs-a /dev/loop0
          #
          # after resilvering we must export the pool.
          #
          # offlining won't be enough as zfs will keep insisting that loop0
          # is the device in the pool and will only realize on reimporting 
          # the pool that another device contain's the pools data.
          # 
          # since this is the pool from which I boot, I have to restart to
          # reimport it, so this is a bit more complicated. But let's assume
          # I booted from a rescue disk, in which case I can export the pool:
          #
          # :: zpool export zmirror-sysfs
          #
          # and then I reimport it giving it the parent direct directory 
          # (or directories) under which to look for devices:
          #
          # :: zpool import zmirror-sysfs -d /dev/zvol/zmirror-bak-a/sysfs -d /dev/mapper
          #
          # As a result the zpool will know the zdev as `zvol/zmirror-bak-a/sysfs`
          # This is the ONLY name that zmirror can work with. And it also 
          # makes sense because human readable names make such a complex
          # disk layout manageable.
          #
          - !ZDev
            pool: zmirror-sysfs

            on_resilvered:
              # when a resilver is done, we want to snapshot the zfs volume
              # backing the zpool
              - snapshot-parent

              # VERY IMPORTANT
              # 
              # zmirror-bak-a is backed by a rotating magnetic disk which
              # is rather slow compared to the NVMe disks that back
              # zmirror-sysfs or the SSDs that back zmirror-big.
              # 
              # This is no problem during resilver as any WRITE issued for
              # the zmirror-sysfs zpool will only go to the disks that are not 
              # resilvering.
              #
              # But the moment that the resilver finishes, the WRITE speed
              # of the WHOLE zpool would drop down to the speed of the rotating 
              # disk. To avoid this, we take the backing blockdev offline right
              # away.
              # 
              # Since my backup disks are all connected via USB, this kind of
              # configuration gives me the advantage of being able to see whether the
              # disk is still online: whenever a resilver or scrubbing is running
              # the data LED on my disk enclosures will blink. The moment it stops
              # blinking I know that zmirror has offlined the disk and I can
              # safely remove it, without sending the zfs kernel module 
              # into an unrecoverable state (zfs currently cannot handle the last disk of 
              # a pool disappearing without freezing the pool until a reboot happens.
              # Onlining the disk again after reconnecting it does not work either)
              - offline

            # above mentioned problem is unavoidable during a scrub, since for scrubbing
            # this blockdev must be online within the zmirror-sysfs zpool. 
            # Consequently we do scrubbing at night when lowered pool speed
            # is less of an issue. And to avoid keeping the 
            # zmirror-sysfs zpool's WRITE speed as low as the speed of the 
            # rotating disk (which is backing zmirror-bak-a) we take the
            # blockdev offline right away.
            # 
            on_scrubbed:
              - offline

            on_appeared:
              - online

            scrub_interval: 4 weeks
      
      # the note from zmirror-bak-a/sysfs applies
      - !ZFSVolume
        name: big
        
        on_children_offline: 
          - offline

        on_appeared:
          - online
        
        content:
          
          - !ZDev
            
            pool: zmirror-big

            on_appeared:
              - online

            on_scrubbed: 
              - offline

            on_resilvered: 
              - snapshot-parent
              - offline

            scrub_interval: 4 weeks







  # zmirror-bak-a is my first backup disk (I actually have more than are 
  # configured in this example). These backup disks are physically distributed
  # when stored (not currently connected to my server that is I store them in 
  # multiple locations outside my home. In this way I mitigate the risk of a local 
  # catastrophy such as a fire.
  #
  # This disk contains but a single zpool (which is backed only by this disk)
  # The consequence is that I cannot scrub this pool. Hence this particular backup 
  # disk is not protected against bitrot. I minimize the risk of bitrot by storing 
  # the disk in an EMP protected bag which protects the disk against 
  # electromagnetic radiation in general (one of the causes of bitrot) and not just
  # against EMPs.
  #
  # and this even protects older data, since both the sysfs and the big pool
  # actually do contain (a limited number of) snapshots of older data that I curate manually.
  # And it contains daily, weekly, monthly and yearly snapshots that are curated automatically
  # by my nixOS
  #
  # Zmirror issues a scrub of the zmirror-sysfs and the zmirror-big pools when
  # this device is connected and at least 4 weeks have passed. This means that
  # the newest data on this backup disk will be scrubbed and thus checked against
  # the life system. Hence bitrot can only affect older snapshots of the whole
  # backup disk, and never data or snapshots that are inside the zmirror-sysfs
  # and zmirror-big pools.
  #
  # this is the partition that contains the zpool's data
  - !Partition
    name: zmirror-bak-a

    content:
      - !DMCrypt
        name: zmirror-bak-a
        
        key_file: ./test/zmirror-key
        
        on_appeared:
          - online
        
        on_children_offline: 
          - offline

        content: 
          - !ZDev
            
            pool: zmirror-bak-a
          







  # This pool is fully protected against bitrot, since it is backed by 
  # two disks configured as mirrors of each other. 
  # For this to work it is necessary that (at least every now and
  # then) you connect both backing disks at the same time and ensure that
  # they will be scrubbed (as this zmirror config does every 4 weeks as long 
  # as you keep the disks connected over night)
  - !ZPool
    name: zmirror-bak-b

    on_children_offline: 
      - offline

    # This zpool should be taken online if all necessary backing devices
    # have appeared. 
    # 
    # This will happen once I have powered the devices on via the shell
    # command, as the respective udev events are being sent by the kernel.
    #
    # This configuration is not dangerous, because all backing devices
    # are configured as required. So zmirror will only import this pool
    # once all backing disks of the mirror are present.
    on_backing_appeared: 
      - online
    
    # for above event handling to work, we also need to configure the backing:
    backed_by:
      - !Mirror
        devices:
          - name: zmirror-bak-b-alpha
            required: yes
          - name: zmirror-bak-b-beta
            required: yes
    

    content:
      
      - !ZFSVolume
        name: sysfs
        on_children_offline: 
          - offline
        on_appeared:
          - online
        content:

          # the note from zmirror-bak-a/sysfs applies
          - !ZDev
            pool: zmirror-sysfs
            
            on_appeared:
              - online
          
            on_scrubbed:
              - offline

            on_resilvered:
              - snapshot-parent
              - offline
      
            scrub_interval: 4 weeks

      - !ZFSVolume
        name: big
        on_children_offline:
          - offline
        on_appeared:
          - online
        content:

          # the note from zmirror-bak-a/sysfs applies
          - !ZDev
            pool: zmirror-big

            on_appeared:
              - online
          
            on_scrubbed:
              - offline

            on_resilvered:
              - snapshot-parent
              - offline

            scrub_interval: 4 weeks




  # this is the main partition of the first disk that is backing zmirror-bak-b
  #
  # We could have used full disks here, since zfs recommends using full disks
  # to back its pools. But there is no consistent way to name disks, while it
  # is easily possible to name partitions. It simply results in a more easy to
  # understand configuration if you can use names instead of serial numbers.
  # and whatever penalty there might be is negligeable.
  #
  - !Partition
    name: zmirror-bak-b-alpha

    content:
      - !DMCrypt
        name: zmirror-bak-b-alpha
        
        key_file: ./test/zmirror-key

        on_appeared:
          - online

        on_children_offline: 
          - offline

        content: 

          - !ZDev
            pool: zmirror-bak-b
          
            on_appeared:
              - online

            scrub_interval: 4 weeks


  # this is the main partition of the second disk that is backing zmirror-bak-b
  - !Partition
    name: zmirror-bak-b-beta

    content:
      - !DMCrypt
        name: zmirror-bak-b-beta
        
        key_file: ./test/zmirror-key

        on_appeared:
          - online

        on_children_offline: 
          - offline

        content: 

          - !ZDev
            pool: zmirror-bak-b
          
            on_appeared:
              - online
            
            scrub_interval: 4 weeks





  # this zpool is being used to backup my bluray disks
  # 
  # it is the slowest pool I have and it is backed by
  # rotating disks only. The disks are powered on via an internal USB
  # relay board which connects/disconnects the SATA power for the
  # devices on command.
  #
  # And this of course I will only do whenever I need to make another backup 
  # of a bluray disk. Because after creating a backup I tend to reencode
  # the movies into a more spaceefficient format onto the `big` pool.
  # This is because I personally am happy with x246 encoded 1080p videos. 
  # I don't really need 4K... But who knows? So I'll back up my blurays which
  # like all optical media have a tendency of becoming unreadable after
  # some years...
  #
  # blubak appears in this example config because it shows how 
  # to combine 2 smaller disks into a single striped disk
  # which can be used to mirror a bigger disk using a zpool mirror
  - !ZPool
    name: zmirror-blubak

    # This zpool should be taken online if all necessary backing devices
    # have appeared. 
    # 
    # This will happen once I have powered the devices on via the shell
    # command, as the respective udev events are being sent by the kernel.
    #
    # *WARNING*: This configuration is dangerous:
    #
    # ZFS uses the disk that is present when the pool was imported as its
    # references point. If you then connect another disk, that was not yet
    # present at pool import time, and if that other disk contained newer 
    # data because it was last used, then that newer data WILL BE LOST.
    # 
    # So you should make sure that you always connect the disk first, that 
    # has the newer data. Or to always have both disks connected, so that
    # when the pool is exported, they always both contain the newest data.
    # Then of course it makes no difference which you connect first.
    on_backing_appeared: 
      - online
    
    # for above event handling to work, we also need to configure the backing:
    backed_by:
      - !Mirror
        devices: 
          - zmirror-blubak-alpha
          - zmirror-blubak-beta




  # this is the main partition of the first (the big) disk that is backing 
  # the zpool zmirror-blubak
  - !Partition
    name: zmirror-blubak-alpha

    content:
      - !DMCrypt
        name: zmirror-blubak-alpha
        
        key_file: ./test/zmirror-key

        on_appeared:
          - online

        on_children_offline: 
          - offline

        content: 

          - !ZDev
            pool: zmirror-blubak
          
            on_appeared:
              - online



  # The second element in the mirror backing zmirror-blubak is more complex.
  # 
  # It is a volume in this pool. So a pool is backed by a volume inside another
  # pool, for the following reason:
  # 
  # This pool's specialty is that it is not backed by two mirrored disks
  # but by two striped disks.
  # 
  # This kind of configuration allows you to use a set of old disks that are 
  # not big enough, to together provide a single mirror of a newer disk that
  # is big enough.
  - !ZPool
    name: zmirror-blubak-beta

    content:
      
      - !ZFSVolume
        name: blubak-beta
        on_children_offline: 
          - snapshot
          - offline
        content:

          - !ZDev
            pool: zmirror-blubak
            
            on_appeared:
              - online
          
            on_scrubbed:
              - offline

            on_resilvered:
              - snapshot-parent
              - offline

    # we want this pool to be imported if all necessary backing devices have appeared
    # This configuration is not dangerous, because the backing does not contain
    # zmirrors or raid-z.
    on_backing_appeared: 
      - online

    # and for zmirror to do that, we need to tell it how the backing is layed out
    backed_by:
      - "zmirror-blubak-beta-05a"
      - "zmirror-blubak-beta-05b"



  # this is the main partition of the second disk that is backing the zpool
  # zmirror-blubak
  # 
  # the speciality of the second and third disk is, that they are (about) half as 
  # big as the first disk, and so we combined them into a striped raid0.
  - !Partition
    name: zmirror-blubak-beta-05a

    content:
      - !DMCrypt
        name: zmirror-blubak-beta-05a

        key_file: ./test/zmirror-key

        on_appeared:
          - online

        on_children_offline: 
          - offline

        content: 

          - !ZDev
            pool: zmirror-blubak-beta


  - !Partition
    name: zmirror-blubak-beta-05b

    content:
      - !DMCrypt
        name: zmirror-blubak-beta-05b

        
        key_file: ./test/zmirror-key

        on_appeared:
          - online

        on_children_offline: 
          - offline

        content: 

          - !ZDev
            pool: zmirror-blubak-beta

